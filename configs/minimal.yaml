experiment:
    benchmark: 'cifar10-split'
    n_tasks: 5
    model: 'mlp'
    strategy: 'er'
    n_classes: 10
    seed: 42

train:
    optimizer: 'adam'
    loss: 'cross_entropy'
    lr: 0.03

    batch_size: 2
    epochs: 2

strategy:
    mem_size: 50
    batch_size_mem: 2
    patch_size: 28
    stride: 2

run_id: 'er_cifar10'

setup:
    checkpoint_dir: '../checkpoints'
    checkpoint_every: 1 # Create a checkpoint every n epochs: -1 means you checkpoint just at the end of each task
